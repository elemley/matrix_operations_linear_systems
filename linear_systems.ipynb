{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Solution of Systems of Linear Equations\n",
    "\n",
    "## Background\n",
    "\n",
    "### Engineering Problems\n",
    "\n",
    "We can write many engineering problems as systems of linear equations, which happen to have many ways of being solved numerically. Here are some general engineering problems that can be written as linear systems:\n",
    "\n",
    "1. Truss Problems (Newton's Laws)\n",
    "2. Circuit Problems (Kirchoff's Laws)\n",
    "3. Motion of Objects with Internal Connections (Newton's laws - written as ODEs)\n",
    "4. Stress/Strain/Deformation (PDEs)\n",
    "5. Fluid Motion (PDEs)\n",
    "\n",
    "### Cramer's Rule\n",
    "\n",
    "When you have a smaller problem, you can usually use Cramer's Rule to solve it. The problem we want to solve is:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\n",
    "\\label{eq1}\\tag{1} $\n",
    "\n",
    "Cramer's Rule States:\n",
    "\n",
    "$ \\large x_0 =  \\frac{\\begin{vmatrix} b_0 & a_{01} & a_{02} \\\\ b_1 & a_{11} & a_{12} \\\\ b_2 & a_{21} & a_{22} \\end{vmatrix}}{\\begin{vmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{vmatrix} }~~~  x_1 =  \\frac{\\begin{vmatrix} a_{00} & b_0 & a_{02} \\\\ a_{10} & b_1 & a_{12} \\\\ a_{20} & b_2 & a_{22} \\end{vmatrix}}{\\begin{vmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{vmatrix} } ~~~  x_2 =  \\frac{\\begin{vmatrix} a_{00} & a_{01} & b_0 \\\\ a_{10} & a_{11} & b_1 \\\\ a_{20} & a_{21} & b_2 \\end{vmatrix}}{\\begin{vmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{vmatrix} }  \\label{eq2}\\tag{2} $\n",
    "\n",
    "I will try to stick with $0$ as the starting subscript. Since we are coding in python that will work much better for the arrays and loop bounds.\n",
    "\n",
    "### Triangular Forms\n",
    "\n",
    "#### Upper Triangulars\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ 0 & a_{11} & a_{12} \\\\ 0 & 0 & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq3}\\tag{3}$\n",
    "\n",
    "An upper triangular problem like this can be solved using *back substitution*.\n",
    "\n",
    "Although not as common another upper triangular can be written as:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & 0 \\\\ a_{20} & 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq4}\\tag{4}$\n",
    "\n",
    "#### Lower Triangulars\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & 0 & 0 \\\\ a_{10} & a_{11} & 0 \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq5}\\tag{5}$\n",
    "\n",
    "And yet again there is an alternative (somewhat uncommon) way of writing another lower triangular:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 0 & 0 & a_{02} \\\\ 0 & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq6}\\tag{6}$\n",
    "\n",
    "#### Diagonal Matrix\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & 0 & 0 \\\\ 0 & a_{11} & 0 \\\\ 0 & 0 & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq7}\\tag{7}$\n",
    "\n",
    "A diagonal matrix is easy to solve because $ \\large x_i = \\frac{b_i}{a_{ii}}$.\n",
    "\n",
    "A very special diagonal matrix is the identity matrix:\n",
    "\n",
    "$ \\large I = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "## Back Substitution - Algorithm\n",
    "\n",
    "The technique known as *back substitution* can be used to find $[x]$ in the case of Eq.($\\ref{eq3}$) - which we will now demonstrate. For Eq.($\\ref{eq3}$) from the last row of the matrix we can write:\n",
    "\n",
    "$\\large x_2 = \\frac{b_2}{a_{22}} \\label{eq8}\\tag{8}$. Since $x_2$ is now known, we can find $x_1$ as\n",
    "\n",
    "$\\large x_1 = \\frac{b_1 - a_{12} x_2}{a_{11}} \\label{eq9}\\tag{9} $ and now we can find $x_0$ as\n",
    "\n",
    "$\\large x_0 = \\frac{b_0 - a_{01} x_1 - a_{02} x_2}{a_{00}} = \\frac{b_0 - (a_{01} x_1 + a_{02} x_2)}{a_{00}} = \\frac{b_0 - \\sum\\limits_{j=1}^{2} a_{0j} x_j}{a_{00}} \\label{eq10}\\tag{10} $\n",
    "\n",
    "For that matter, $x_1$ could have been written as \n",
    "\n",
    "$\\large x_1 = \\frac{b_1 - \\sum\\limits_{j=2}^{2} a_{1j} x_j}{a_{11}} \\label{eq11}\\tag{11} $\n",
    "\n",
    "Of course there will be only one term in the summation here. So we now almost have an algorithm... we just need to generalize for $x_i$ and determine the generalized summation limits. We will just use the simple calculation of $x_2$ to get the process started and that the size of the problem/martrix is $nxn$.\n",
    "\n",
    "In looking at Eq.($\\ref{eq10}$) where $i=0$ it seems that *j* ranges from $i+1 = 1$ to $n-1 = 2$ So the general form of the expression for finding $x_i$ would be\n",
    "\n",
    "$\\large x_i =  \\frac{b_i - \\sum\\limits_{j=i+1}^{n-1} a_{ij} x_j}{a_{ii}}~~~ i=n-2, n-3, ...0 \\label{eq12}\\tag{12} $\n",
    "\n",
    "Again, this assumes that we have already calculated:\n",
    "\n",
    "$\\large x_{n-1} = \\frac{b_{n-1}}{a_{n-1,n-1}} \\label{eq13}\\tag{13}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Elimination (including Forward Elimination, Normalization and Back Substitution)\n",
    "\n",
    "Returning to the original problem:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix} $\n",
    "\n",
    "Now we will demonstrate the Gauss elimination technique with an example. The *pivots* are the diagonal elements and a *pivot* row is a row containing a pivot. The process starts at the top row, eliminates all elements below the current pivot, then moves to the next row.\n",
    "\n",
    "$ \\large x_1 - 3 x_2 + x_3 = 4 $\n",
    "\n",
    "$ \\large 2 x_1 - 8 x_2 + 8 x_3 = -2 $\n",
    "\n",
    "$ \\large -6 x_1 + 3 x_2 - 15  x_3 = 9 $\n",
    "\n",
    "Or written in matrix form.\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 \\\\ 2 & -8 & 8 \\\\ -6 & 3 & -15 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 9 \\end{bmatrix} $\n",
    "We usually write this in an abbreviated form that is convenient for Gauss Elimination: \n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 2 & -8 & 8 & -2 \\\\ -6 & 3 & -15 & 9 \\end{bmatrix}$\n",
    "\n",
    "This is the *augmented matrix* form. It's convenient because we are going to do operations on each row (including the RHS vector).\n",
    "\n",
    "### Forward Elimination\n",
    "\n",
    "The process is basically this: multiply the pivot row by negative of the element below the pivot you want to eliminate... then add this multiplied pivot row to the row in which you are eliminating an element... the result of this addition is the new row. For example if the pivot is the *1* in position $a_{00}$, then to get rid of the *2* in position $a_{10}$, we would multiply row *0* by *-2* then add that to row *1*. Below I will use an $R_i$ to represent the ith row.\n",
    "\n",
    "$ \\large -2 R_0 + R_1 = R_{1}^{'} $\n",
    "\n",
    "$ \\large -2 \\begin{bmatrix} 1 & -3 & 1 & 4 \\end{bmatrix} = \\begin{bmatrix} -2 & 6 & -2 & -8 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -2 & 6 & -2 & -8 \\end{bmatrix} + \\begin{bmatrix} 2 & -8 & 8 & -2 \\end{bmatrix} = \\begin{bmatrix} 0 & -2 & 6 & -10 \\end{bmatrix} = R_{1}^{'}$\n",
    "\n",
    "Now the augmented matrix looks as follows:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & -2 & 6 & -10 \\\\ -6 & 3 & -15 & 9 \\end{bmatrix}$\n",
    "\n",
    "Now we can eliminate the *-6* in $R_2$ by the following operation: $ 6 R_0 + R_2 = R_{2}^{'} $, which gives:\n",
    "\n",
    "$ \\large 6 \\begin{bmatrix} 1 & -3 & 1 & 4 \\end{bmatrix} = \\begin{bmatrix} 6 & -18 & 6 & 24 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 6 & -18 & 6 & 24 \\end{bmatrix} + \\begin{bmatrix} -6 & 3 & 15 & 9 \\end{bmatrix} = \\begin{bmatrix} 0 & -15 & -9 & 33 \\end{bmatrix} = R_{2}^{'}$\n",
    "\n",
    "So the new version of the augmented matrix looks like:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & -2 & 6 & -10 \\\\ 0 & -15 & -9 & 33 \\end{bmatrix}$\n",
    "\n",
    "The next step is use $R_{1}^{'}$ as the pivot row, but before doing that we *normalize* $R_{1}^{'}$... which means to make the pivot element - the *-2* - a *1* ... which we can do by taking $\\frac{R_{1}^{'}}{-2} = \\begin{bmatrix} 0 & 1 & -3 & 5 \\end{bmatrix} = R_{1}^{''}$ which leaves our matrix as\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & 1 & -3 & 5 \\\\ 0 & -15 & -9 & 33 \\end{bmatrix}$\n",
    "\n",
    "Now eliminate the *-15* below the current pivot by $ 15 R_{1}^{''} + R_{2}^{'} $, which gives:\n",
    "\n",
    "$ \\large 15 \\begin{bmatrix} 0 & 1 & -3 & 5 \\end{bmatrix} = \\begin{bmatrix} 0 & 15 & -45 & 75 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & 15 & -45 & 75 \\end{bmatrix} + \\begin{bmatrix} 0 & -15 & -9 & 33 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & -54 & 108 \\end{bmatrix} = R_{2}^{''}$\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & 1 & -3 & 5 \\\\ 0 & 0 & -54 & 108 \\end{bmatrix}$\n",
    "\n",
    "Let's go ahead and normalize the last row - $\\frac{R_{2}^{''}}{-54} = \\begin{bmatrix} 0 & 0 & 1 & -2 \\end{bmatrix} = R_{2}^{'''}$.\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & 1 & -3 & 5 \\\\ 0 & 0 & 1 & -2 \\end{bmatrix}$\n",
    "\n",
    "At this point *forward elimination* is complete!\n",
    "\n",
    "### Back Substitution\n",
    "\n",
    "The equation form of the last row $R_{2}^{'''}$ is:\n",
    "\n",
    "$ \\large (1)x_2 = -2 \\rightarrow x_2 = -2 $\n",
    "\n",
    "Allowing us to solve the equation form of $R_{1}^{''}$:\n",
    "\n",
    "$ \\large (1) x_1 + (-3)(-2) = 5 \\rightarrow x_1 = -1$\n",
    "\n",
    "And finally we can solve the equation from $R_{0}^{'}$ as\n",
    "\n",
    "$ \\large (1) x_0 + (-3)(-1) + (1)(-2) = 4  \\rightarrow x_0 = 3$\n",
    "\n",
    "Giving the final solution as:\n",
    "\n",
    "$ \\large x = \\begin{bmatrix} 3 \\\\ -1 \\\\ -2 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-integer example\n",
    "\n",
    "We will now do an example that cannot be done only with integers... in fact we will limit the number of decimal places so we can see the effects.\n",
    "\n",
    "$ \\large \\begin{bmatrix} 0.143 & 0.357 & 2.01 \\\\ -1.31 & 0.911 & 1.99 \\\\ 11.2 & -4.30 & -0.605 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} -5.173 \\\\ -5.458 \\\\ 4.415 \\end{bmatrix} $ In augmented form: $ \\large \\begin{bmatrix} 0.143 & 0.357 & 2.01 & -5.173\\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} $\n",
    "\n",
    "\n",
    "Now the steps of Gauss Elimination would go as follows:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 4.19 & 20.5 & -52.9 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 4.19 & 20.5 & -52.9 \\\\ 0.00 & -32.3 & 159. & 409 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 1.00 & 4.89 & -12.6 \\\\ 0.00 & -32.3 & 159. & 409 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 1.00 & 4.89 & -12.6 \\\\ 0.00 & 0.00 & -1.00 & 2.00 \\end{bmatrix} $\n",
    "\n",
    "And finally:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 1.00 & 4.89 & -12.6 \\\\ 0.00 & 0.00 & 1.00 & -2.00 \\end{bmatrix} $\n",
    "\n",
    "Now backsubstition:\n",
    "\n",
    "$ \\large  x_2 = -2.00~~~ x_1 + 4.89(-2.0) = -12.6 \\rightarrow x_1 = -2.82~~~ x_0 + 2.50(-2.82) + 14.1(-2.00) = -36.2 \\rightarrow x_0 = -0.950 $\n",
    "\n",
    "Like a good engineer - we should check our answers, but we should use our original equations:\n",
    "\n",
    "\n",
    "$ \\large 0.143(-0.950) + 0.357(-2.82) + 14.1(-2.00) = -29.3 \\ne -5.173$ Oops... this is not a good sign.\n",
    "\n",
    "$ \\large -1.31(-0.950) + 0.911(-2.82) + 1.99(-2.00) = -5.30 \\ne -5.458$\n",
    "\n",
    "$ \\large 11.2(-0.950) + (-4.30)(-2.82) + (-0.605)(-2.00) =  2.70 \\ne 4.415$\n",
    "\n",
    "So - what happened? The limited precision of the numbers throughout the process caused significant rounding error. Although we kept the rounding to three digits - a pretty significant loss of precision - we were only working a 3x3 problem. Imagine the effects one could encounter in a much larger system of equations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Elimination Algorithm\n",
    "\n",
    "As understandable as the forward elimination algorithm might seem to be in doing a problem by hand... to automate the process (and then finally write code to implement it) is not trivial. To help understand how you can take a complicated process and automate it, it's often easier to write out an example problem - but to start to understand what parts need to be repeated and since there will be indicies for the array items we need to understand the patterns that appear in the algorithm with indicies. To do this here is a written document that shows how to think through the process for *forward elimination.*\n",
    "\n",
    "[Forward Elimination Algorithm Development](forward_elim_alg_dev.pdf).\n",
    "\n",
    "Note that the algorithm development document does not address how to move from one row to the next as the pivot row changes... it also does not address normalizing these additional pivot rows as we move down through the matrix.\n",
    "\n",
    "This assumes we are starting our indices at 0 and the original problem to be solved is $n~x~n$. Also this assumes we have augmented the nxn matrix with the RHS vector, so the matrix we are working on is actually $n~x~(n+1)$.\n",
    "\n",
    "- *i* index for pivot rows... $ i=0,1,...,n-2$ (Note the row index $n-2$ is the next to last row... and that is where we stop the elimination step).\n",
    "     - *j* index is for rows below *i* in which we are eliminating. $ j=i+1, i+2, ..., n-1 $\n",
    "         - *k* indicates the column in which we are *modifying* elements within row *j*. $ k = i, i+1, i+2, ... , n$\n",
    "             - The replacement/modified element in row *j* for each *k* is calculated as\n",
    "                 $\\large a_{jk} = a_{jk} - a_{ji} a_{ik} $\n",
    "                 - Note that here that the $a_{jk}$ that appears on the RHS of this expression is the *old* value of $a_{jk}$ that we are replacing. In code form this expression will be:\n",
    "                 - ```python\n",
    "                 a[j][k] -= a[j][i]*a[i][k]\n",
    "                 ```\n",
    "             - This process needs to be done all the way to $k=n$ since we are operating on the augmented matrix, which has $n+1$ columns.\n",
    "\n",
    "The above algorithm suggests a series of three nested loops. We do have a missing piece... *normalization* of the pivots. So for the moment let's assume we have written a function to do the normalization of row *i*. The *pseudo-code* below will not actually run in python... it's just a step closed to the actual program.\n",
    "\n",
    "```python\n",
    "#ab = defined as the augmented matrix\n",
    "for i = 0 to n-2\n",
    "    normalize(ab,i)\n",
    "    for j = i+1 to n-1\n",
    "        for k = i to n\n",
    "            #note we may have issues if we start at k = i...\n",
    "            #k=i is the column where the element is being eliminated\n",
    "            #and the following step may not make the new element exactly zero...\n",
    "            ab[j][k] -= ab[j][i]*ab[i][k]\n",
    "```\n",
    "An alternative to get around the issue of the element to be eliminated not being exactly zero is:\n",
    "\n",
    "```python\n",
    "#ab = defined as the augmented matrix\n",
    "for i = 0 to n-2\n",
    "    normalize(ab,i)\n",
    "    for j = i+1 to n-1\n",
    "        ab[j][i] = 0.0 #force the element you are trying to eliminate to be exactly 0.0\n",
    "        for k = i+1 to n\n",
    "            ab[j][k] -= ab[j][i]*ab[i][k]\n",
    "```\n",
    "Either technique can be used and it will make only small differences (keep in mind those small differences might add up to bigger differences if repeated enough times...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Elimination with Partial Pivoting\n",
    "\n",
    "When we worked the following example, it was discussed that we ideally want pivot elements that have the largest magnitude as possible... this because in advance we know we will be dividing the entire pivot row by this value to normalize... *and as we have discussed many times... we never knowingly divide by a small number!!* So what we really should do is as we move to each new pivot row, to find the best possible row (one with the largest magnitude in the pivot position) and swap rows -- this is called **partial pivoting.**\n",
    "\n",
    "$ \\large \\begin{bmatrix} 0.143 & 0.357 & 2.01 & -5.173\\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 11.2 & -4.30 & -0.605 & 4.415\\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 0.143 & 0.357 & 2.01 & -5.173 \\end{bmatrix}  $\n",
    "\n",
    "After the swap shown above, we would normalize row $0$ then eliminate the elements below our pivot. Note once we move to the next pivot row, we would repeat the process of searching for the best possible pivot at that time. So the partial pivoting process is not done once at the first, but needs to be repeated to find the largest magnitude pivot values each time we shift to a new pivot row.\n",
    "\n",
    "The question is how would we automate this process? There are two parts to this process...\n",
    "\n",
    "1. Identifying the best pivot - searching the pivot column to identify the largest magnitude value,\n",
    "2. \"Swapping\" rows - but should we actually move elements around in the array? The answer is no that would take too much time, in general... instead we will use a vector to keep track of which row we want to \"pretend\" is in each location. We will maintain this pretense with a vector.\n",
    "\n",
    "Here we return to a general form of a $3x3$ problem with the augmented matrix:\n",
    "\n",
    "$ \\large \\begin{bmatrix} ab_{00} & ab_{01} & ab_{02} & ab_{03} \\\\ ab_{10} & ab_{11} & ab_{12} & ab_{13} \\\\ ab_{20} & a_{21} & ab_{22} & ab_{23}\\end{bmatrix} $\n",
    "\n",
    "### Searching for the *Best Value*\n",
    "\n",
    "Let's look specifically at the problem above before any row swapping has occurred. How do you determine which row to switch with? You looked at all elements in the $0$th column and noticed which had the largest magnitude. In pseudocode form:\n",
    "\n",
    "```python\n",
    "max = abs(ab[0][0])   #start by assuming the current pivot is the largest\n",
    "for p = 1 to 2   #look in all rows below the current row\n",
    "    if abs(ab[p][0]) > max\n",
    "        max = abs(ab[p][0])\n",
    "```\n",
    "At the end of this process you would have the correct value in *max,* but you would not have retained which row it came from... so now we have to address keeping track of which row is which.\n",
    "\n",
    "### The *order* Vector\n",
    "\n",
    "We now will make a vector that kept track of the rows. The way it works is that the vector starts as just:\n",
    "\n",
    "$ \\large order=[0, 1, 2] $\n",
    "\n",
    "If we need to swap rows, we will change the $order$ vector to reflect this. For example, for the swap in this example: placing row $0$ in row $2$'s position, the order vector should become:\n",
    "\n",
    "$ \\large order=[2, 1, 0] $\n",
    "\n",
    "So now the index $2$ is in the $0$th position and the index $0$ is in the $2$-row position. How do we accomplish this via pseudocode?\n",
    "\n",
    "```python\n",
    "max = abs(ab[0][0])                    #start by assuming the current pivot is the largest\n",
    "order = [0,1,2]                   #define the order vector\n",
    "index = order[0]                  #this is keeping track of which row is best at the moment\n",
    "for p = 1 to 2                    #look in all rows below the current row\n",
    "    if abs(ab[p][0]) > max\n",
    "        max = abs(ab[p][0])\n",
    "        index = p\n",
    "tmp = order[0]                    #temporarily store order[0] here\n",
    "order[0] = order[index]           #replace order[0] with the best pivot row\n",
    "order[index] =  tmp               #place the original pivot row in the location where the best pivot row had been\n",
    "```\n",
    "\n",
    "The next step is to generalize this so it will work for any pivot row. We have used the index *i* for the current pivot row, so we will continue with this notation now. Note, here we assume the *order* vector is in place (but note, it could have been previously modified).\n",
    "\n",
    "```python\n",
    "max = abs(ab[i][i])                    #start by assuming the current pivot is the largest\n",
    "index = order[i]                  #this is keeping track of which row is best at the moment\n",
    "for p = i+1 to n-1                    #look in all rows below the current row\n",
    "    if abs(ab[p][i]) > max\n",
    "        max = abs(ab[p][i])\n",
    "        index = p\n",
    "tmp = order[i]                    #temporarily store order[0] here\n",
    "order[i] = order[index]           #replace order[0] with the best pivot row\n",
    "order[index] =  tmp               #place the original pivot row in the location where the best pivot row had been\n",
    "```\n",
    "\n",
    "### How Partial Pivoting Works\n",
    "\n",
    "So we now have a pathway to make sure we know which is the best pivot row, but how do we actually use this with *forward elimination*, *normalization*, and *back substitution*? The answer is we keep up the pretense that we have actually swapped rows by dealing with the following matrix (instead of the original we were given):\n",
    "\n",
    "$ \\large \\begin{bmatrix} ab_{order[0],0} & ab_{order[0],1} & ab_{order[0],2} & ab_{order[0],3} \\\\ ab_{order[1],0} & ab_{order[1],1} & ab_{order[1],2} & ab_{order[1],3} \\\\ ab_{order[2],0} & a_{order[2],1} & ab_{order[2],2} & ab_{order[2],3}\\end{bmatrix} $\n",
    "\n",
    "In short, instead of using the actual row index, we use the what is in the *order* vector in the location for the actual row index. So any operation on the augmented matrix, is done with the above version of the matrix. Note the changes to any code for *forward elimination*, *normalization*, and *back substitution* are minimal and amount to replacing row indicies with the *order vector* referenced to the actual row index.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Jordan Elimination\n",
    "\n",
    "If we use forward and *back* elimination combined with normalization, it is possible to transform the original coefficient matrix, $a$ into the identity matrix. Then the modified RHS vector, $b$ will actually be the solution.\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_{0}^{'} \\\\ b_{1}^{'} \\\\ b_{2}^{'} \\end{bmatrix} $\n",
    "\n",
    "So now the $b^{'}$ vector contains the solution.\n",
    "\n",
    "The algorithm to do Gauss-Jordan is very similar to forward elimination - one just needs to do elimination in rows that are *above and below* the pivot row. Note one important attribute of this technique is that *back substitution is not required!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition\n",
    "\n",
    "*Lower - Upper* or LU decomposition is useful for some problems, especially ones in which the same coefficient matrix is used over and over with a different RHS vector. Although there are some different ways of doing LU decomposition we are focused on Crout's method which decomposes the coefficient matrix into a lower triangular and an upper triangular as:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} & a_{03} \\\\ a_{10} & a_{11} & a_{12} & a_{13}\\\\ a_{20} & a_{21} & a_{22} & a_{23}\\\\  a_{30} & a_{31} & a_{32} & a_{33} \\end{bmatrix} = \\begin{bmatrix} L_{00} & 0 & 0 & 0\\\\ L_{10} & L_{11} & 0 & 0 \\\\ L_{20} & L_{21} & L_{22} & 0 \\\\L_{30} & L_{31} & L_{32} & L_{33} \\end{bmatrix}  \\begin{bmatrix} 1 & U_{01} & U_{02} & U_{03} \\\\ 0 & 1 & U_{12} & U_{13} \\\\ 0 & 0 & 1 &U_{23} \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $\n",
    "\n",
    "By carrying out the matrix multiplication on the RHS of the above equation ine can find how the elements of $a$ are related to $L$ and $U$. The details of this are here:\n",
    "\n",
    "[Crout Method Algorithm Development](crout_decomp_alg_dev.pdf).\n",
    "\n",
    "One the attributes of this algorithm is that in must proceed in a certain order to ensure that the correct $L$ and $U$ elements are known in time for calculation of other $L$ and $U$ elements. The last page of the algorithm development document shows this graphically, but putting this into pseudocode:\n",
    "\n",
    "```python\n",
    "for j = 0 to n-1\n",
    "    L[j][0] = a[j][0]\n",
    "for i = 0 to n-1\n",
    "    U[i][i] = 1.0\n",
    "for i = 0 to n-1\n",
    "    for j = 0 to n-1\n",
    "        #if left of the diagonal or on the diagonal\n",
    "            L[i][j] = a[i][j] - get_sum(L,U,i,j)\n",
    "        #if right of the diagonal\n",
    "            U[i][j] = (a[i][j] - get_sum(L,U,i,j))/L[i][i]\n",
    "```\n",
    "In the above pseudocode it is assumed you have a function called *get_sum* that calculates: $ \\large \\displaystyle\\sum_{k-0}^{j-1} L_{ik} U_{kj} $\n",
    "\n",
    "### Using LU Decomposition to solve $[a][x]=[b]$\n",
    "\n",
    "You may have noticed that LU decomposition does not account for finding $[x]$ and that no $[b]$ has been specified. As mentioned LU decomposition is good when the same coefficient matrix needs to be used for a number of different $[b]$ RHS vectors. So here is the way one can find $[x]$ once you have $[L]$ and $[U]$.\n",
    "\n",
    "$ \\large [a][x] = [L][U][x] = [b] \\rightarrow [L][y] = [b]$ where $\\large [U][x] = y$\n",
    "\n",
    "$ \\large \\begin{bmatrix} L_{00} & 0 & 0 & 0\\\\ L_{10} & L_{11} & 0 & 0 \\\\ L_{20} & L_{21} & L_{22} & 0 \\\\L_{30} & L_{31} & L_{32} & L_{33} \\end{bmatrix} \\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\leftarrow $ Solve this for $\\large [y]$.\n",
    "\n",
    "\n",
    "Note $[y]$ can be found by forward elimination -- or better yet a form of *forward substitution* can be used. This is completely analogous to using back substitution, but done from the top to the bottom of the matrix.\n",
    "\n",
    "\n",
    "Now that we have $[y]$:\n",
    "\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & U_{01} & U_{02} & U_{03} \\\\ 0 & 1 & U_{12} & U_{13} \\\\ 0 & 0 & 1  & U_{23} \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix}   \\leftarrow $ Solve this for $\\large [x]$.\n",
    "\n",
    "The above equation can solved quickly with back substitution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inversion\n",
    "\n",
    "The matrix inverse, $[a]^{-1}$ is defined as:\n",
    "\n",
    "$ \\large [a]^{-1} [a] = [I]$\n",
    "\n",
    "When a matrix inverse is possible to calculate it can be very convenient, but please note: *matrix inversion is fraught with issues related to how much precision can be retained in the calculation when $\\det{a} = 0$ or $\\det{a}$ is relatively small.*\n",
    "\n",
    "There are several techniques used to perform matrix inversion numerically. We will present only one here (without proof). This technique uses Gauss-Jordan Elimination. First $[a]$ is augmented with an indentity matrix of the same size as $[a]$ then Gauss Jordan is used until the original elements of $[a]$ are filled with an identity matrix as shown:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} & a_{03} & 1 & 0 & 0 & 0 \\\\ a_{10} & a_{11} & a_{12} & a_{13} & 0 & 1 & 0 & 0\\\\ a_{20} & a_{21} & a_{22} & a_{23} & 0 & 0 & 1 & 0 \\\\  a_{30} & a_{31} & a_{32} & a_{33} & 0 & 0 & 0 & 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix}  1 & 0 & 0 & 0 & a_{00}^{'} & a_{01}^{'} & a_{02}^{'} & a_{03}^{'} \\\\  0 & 1 & 0 & 0 & a_{10}^{'} & a_{11}^{'} & a_{12}^{'} & a_{13}^{'}\\\\  0 & 0 & 1 & 0 &  a_{20}^{'} & a_{21}^{'} & a_{22}^{'} & a_{23}^{'} \\\\  0 & 0 & 0 & 1 &  a_{30}^{'} & a_{31}^{'} & a_{32}^{'} & a_{33}^{'} \\end{bmatrix}   $\n",
    "\n",
    "The *primed* elements above are the elements of $[a]^{-1}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Methods\n",
    "\n",
    "Iterative methods do not directly try to solve the original $[a][x]=[b]$ problem using a one-step solution technique like elimination or decomposition. These methods instead use an *initial guess of the solution* $[x]$. The initial guess may be a very good estimate of $[x]$ or if nothing is known about the solution, the initial guess might be pretty crude, for example guessing that all of the values in $[x]$ are the same number. The way these methods proceed after the inital guess is made, is to start finding new values in $[x]$ based on the previous ones. All of the equations that make up the linear system are solved for their pivot value (i.e. the x-value on the diagonal in a given row. So for a $4x4$ system this might look like:\n",
    "\n",
    "$ \\large x_0 = [b_0 - (a_{01} x_1 + a_{02} x_2 + a_{03} x_3)] / a_{00} $\n",
    "\n",
    "$ \\large x_1 = [b_1 - (a_{10} x_0 + a_{12} x_2 + a_{13} x_3)] / a_{11} $ etc...\n",
    "\n",
    "In general:\n",
    "\n",
    "$ \\large x_i = \\frac{b_i - \\displaystyle\\sum_{j=0,j \\ne i}^{j=n-1}{a_{ij} x_j} }{a_{ii}} ~~~~ i=0,1,2,...n-1$ \n",
    "\n",
    "The equation looks a lot like back substitution. The general idea is that after one calculates all $x_i$ values for one iteration, you simply move on to an another iteration - unless some stopping criterion has been met. More about this soon.\n",
    "\n",
    "The good thing about this iterative technique is that it is very fast. The pitfall is that the algorithm might not converge. In this technique and many other linear system algorithms it is best if the matrix $[a]$ is diagonally dominant, which means:\n",
    "\n",
    "$ \\large |a_{ii}| > \\displaystyle\\sum_{j=0,j \\ne i}^{j=n-1}{ |a_{ij}| } $\n",
    "\n",
    "There might need to be considerable rearrangement of $[a]$ to approach this condition, but the more this condition is true the more likely the process will be to converge. Another factor in the time to convergence is how close is the initial guess of $[x]$ to the true solution.\n",
    "\n",
    "### Jacobi Method\n",
    "\n",
    "Let's assume that the intial guess for $[x]$ has been made. We will call the initial guess vector $[x]^{(0)}$ and each element of the guess $x_{i}^{(0)}$. The iterations proceed and the next iteration results would be $x_{i}^{(1)}$ then $x_{i}^{(2)}$ etc. So assuming the previous iteration has just concluded was iteration $k$, then the next iteration is found as:\n",
    "\n",
    "$ \\large x_i^{(k+1)} = \\frac{b_i - \\displaystyle\\sum_{j=0,j \\ne i}^{j=n-1}{a_{ij} x_j^{(k)}} }{a_{ii}} ~~~~ i=0,1,2,...n-1$ \n",
    "\n",
    "So all $x_i^{k+1}$ values are calculated using the previous iteration's results $x_i^{k}$.\n",
    "\n",
    "This process continues until all $x_i$ meet a stopping criterion, $\\epsilon_s$ as determined by calculating the relative fractional error:\n",
    "\n",
    "$ \\epsilon_r = \\large \\bigg| \\frac{x_i^{(k+1)} - x_i^{(k)} }{x_i^{(k+1)}} \\bigg| < \\epsilon_s $\n",
    "\n",
    "Note the above condition has be met for all $x_i^{(k+1)}$. \n",
    "\n",
    "### Gauss Siedel\n",
    "\n",
    "The only difference Jacobi and Gauss Siedel (GS) is that GS makes use of any newly obtained $x_i$ values in finding the $x_i^{(k+1)}$ values. For example when finding $x_2^{(k+1)}$ one would already know $x_0^{(k+1)}$ and $x_1^{(k+1)}$ and since these values (if the process is converging) will be better than the *kth* iteration -  the GS method makes use of the newest and hopefully best possible values of $x_i$. This does complicate the algorithm a little bit, but not too much.\n",
    "\n",
    "To make sure this is clear, let's say that we have a $5x5$ problem and we have already found $x_0^{(k+1)} and x_1^{(k+1)}$, but now it's time to calculate $x_2^{(k+1)}$. Here is how we would do this:\n",
    "\n",
    "$ \\large x_2^{(k+1)} = \\frac{b_i - ( a_{20} x_0^{(k+1)} + a_{21} x_1^{(k+1)} + a_{23} x_3^{(k)}+a_{24} x_4^{(k)}) }{a_{22}} $ \n",
    "\n",
    "So, we use the $ x_i^{k+1}$ values when available and the $ x_i^{k}$ otherwise. This can be done via either logic or summation functions. Here is the official Gauss Siedel Algorithm:\n",
    "\n",
    "$ \\large x_i^{(k+1)} = \\frac{b_i - \\displaystyle\\sum_{j=0}^{j=i-1}{a_{ij} x_j^{(k+1)} - \\displaystyle\\sum_{j=i+1}^{j=n-1}{a_{ij} x_j^{(k)} } } }{a_{ii}} ~~~~ i=0,1,2,...n-1$ \n",
    "\n",
    "This process continues until all $x_i$ meet a stopping criterion, $\\epsilon_s$ as determined by calculating the relative fractional error:\n",
    "\n",
    "$ \\epsilon_r = \\large \\bigg| \\frac{x_i^{(k+1)} - x_i^{(k)} }{x_i^{(k+1)}} \\bigg| < \\epsilon_s $\n",
    "\n",
    "Note the above condition has to be met for all $x_i^{(k+1)}$. \n",
    "\n",
    "Here is ths pseudocode for Gauss Siedel, note this assumes $[b]$ has been augmented to $[a]$ and the result is $[ab]$ and that you have a function called err_check, that takes $x^{(k+1)}$, $x^{(k)}$, and the stopping criterion and returns *true* if all values of $x^{(k+1)}$ have a relative error less than the stopping criterion.\n",
    "\n",
    "```python\n",
    "#ab, n, and x_k are defined\n",
    "#x_k is a numpy array/vector containing the n initial guesses for the solution\n",
    "max_iter = 100\n",
    "err_stop = 1e-6\n",
    "x_k_plus_1 = np.zeros(n)\n",
    "for p = 0 to max_iter-1\n",
    "    for i = 0 to n-1\n",
    "        summ = 0\n",
    "        for j = 0 to n-1\n",
    "            if j < i\n",
    "                summ += ab[i][j]*x_k_plus_1[j]\n",
    "            elif j > i\n",
    "                summ += ab[i][j]*x_k[j]\n",
    "        x_k_plus_1[i] = (ab[i][n] - summ)/ab[i][i]\n",
    "    if err_check(x_k, x_k_plus_1, err_stop)\n",
    "        break\n",
    "    for j = 0 to n-1\n",
    "        x_k[j] = x_k_plus_1[j]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error, Residual, Norms, and Condition Number\n",
    "\n",
    "### Error and Residual\n",
    "\n",
    "Suppose we have solved $[a][x]=[b]$ using one the algorithms described in this module and gotten a *numerical solution*, which we will call $[x_{NS}]$. We need a way to understand how far we might expect our results to be from the *true solution*, which will be called $[x_{TS}]$. If we actually knew $[x_{TS}]$, we could just calculate the *error vector*, $[e]$, which is defined as\n",
    "\n",
    "$ \\large [e] = [x_{TS}] - [x_{NS}] $\n",
    "\n",
    "We can't usually calculate $[e]$ since we don't usually know $[x_{TS}]$. But there are ways of estimating it - more about this a little later.\n",
    "\n",
    "There is one quantity we can calculate, and that is the *residual*. The residual, $[r]$ is the difference between the known $[b]$ and the result of calculating $[a][x_{NS}]$, which is kind of the $[b]$ that results when we use $[x_{NS}]$ versus $[x_{TS}]$.\n",
    "\n",
    "$\\large [r] = [b] - [b_{NS}] = [a][x_{TS}] - [a][x_{NS}] $\n",
    "\n",
    "Note there is a clear relationship between $[r]$ and $[e]$ which we can see by factoring out $[a]$ in the last expression above:\n",
    "\n",
    "$\\large [r] = [a][x_{TS}] - [a][x_{NS}]  = [a][e]$\n",
    "\n",
    "So, in effect, we can know something about the error from calculating the residual.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 0.143 & 0.357 & 2.01 \\\\ -1.31 & 0.911 & 1.99 \\\\ 11.2 & -4.30 & -0.605 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} -5.173 \\\\ -5.458 \\\\ 4.415 \\end{bmatrix}$\n",
    "\n",
    "Let's say that we get the following result:\n",
    "\n",
    "$ \\large  x_{NS} = \\begin{bmatrix} -0.950 \\\\ -2.82 \\\\ -2.00 \\end{bmatrix} $\n",
    "\n",
    "In this case we do happen to know the *true solution* is:\n",
    "\n",
    "$ \\large  x_{TS} = \\begin{bmatrix} 1.00 \\\\ 2.00 \\\\ -3.00 \\end{bmatrix} $\n",
    "\n",
    "So we can (just this time since we know $x_{TS}$ find $[e]$\n",
    "\n",
    "$ \\large [e] = \\begin{bmatrix} 1.00 \\\\ 2.00 \\\\ -3.00 \\end{bmatrix} - \\begin{bmatrix} -0.950 \\\\ -2.82 \\\\ -2.00 \\end{bmatrix}  = \\begin{bmatrix} 1.95 \\\\ 4.82 \\\\ -1.00 \\end{bmatrix} $\n",
    "\n",
    "Assessing how good or bad this error vector is is a matter of looking at the scale of the elements in $[e]$ compared to the original elements in $[a]$ and $[b]$.\n",
    "\n",
    "We can also get the residual, but first let's find $[b_{NS}]$:\n",
    "\n",
    "$ \\large [b_{NS}] = \\begin{bmatrix} 0.143 & 0.357 & 2.01 \\\\ -1.31 & 0.911 & 1.99 \\\\ 11.2 & -4.30 & -0.605 \\end{bmatrix} \\begin{bmatrix} -0.950 \\\\ -2.82 \\\\ -2.00 \\end{bmatrix} = \\begin{bmatrix} (0.143)(-0.950) + (0.357)(-2.82) + (2.01)(-2.00) \\\\ (-1.31)(-0.950) + (0.911)(-2.82) + (1.99)(-2.00) \\\\ (11.2)(-0.950) + (-4.30)(-2.82) + (-0.605)(-2.00) \\end{bmatrix} = \\begin{bmatrix} -5.16 \\\\ -5.30 \\\\ 2.70 \\end{bmatrix} $\n",
    "\n",
    "So, the $[r]$ is:\n",
    "\n",
    "$ \\large [r] = \\begin{bmatrix} -5.173 \\\\ -5.458 \\\\ 4.415 \\end{bmatrix} - \\begin{bmatrix} -5.16 \\\\ -5.30 \\\\ 2.70 \\end{bmatrix}  = \\begin{bmatrix} -0.013  \\\\ -0.158  \\\\ 1.72 \\end{bmatrix} $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms and Condition Numbers\n",
    "\n",
    "It's easy to understand the scale of a scalar number using absolute values. The scale of a matrix is harder to assess and there are many ways to describe this. A *norm* is a way to do this. A norm must satisfy the following:\n",
    "\n",
    "1. The norm is designated: $\\begin{Vmatrix}[a]\\end{Vmatrix}$ and is positive and only equal to zero, if all the elements of the\n",
    "matrix are zero,\n",
    "2. If you scale a matrix up by multiplying it by a scalar number, $\\alpha$, the norm should also scale up by $\\alpha$:\n",
    "    - $\\large\\begin{Vmatrix} \\alpha [a] \\end{Vmatrix} = \\alpha\\begin{Vmatrix}[a]\\end{Vmatrix} $, \n",
    "3. $ \\large \\begin{Vmatrix}[a][x]\\end{Vmatrix} \\le \\begin{Vmatrix}[a]\\end{Vmatrix} \\begin{Vmatrix}[x]\\end{Vmatrix} $,\n",
    "4. Finally, $ \\large \\begin{Vmatrix}[a]+[b]\\end{Vmatrix} \\le \\begin{Vmatrix}[a]\\end{Vmatrix} + \\begin{Vmatrix}[b]\\end{Vmatrix}$.\n",
    "\n",
    "There some good reasons to have these rules concerning the definitions of norms, but that is beyond the scope of this course.\n",
    "\n",
    "### Calculating Vector Norms\n",
    "\n",
    "Assume we have a vector $[v]$. Below we go through some norms and show an example calculation of each. For these examples we will use\n",
    "\n",
    "$ \\large [v] = \\begin{bmatrix}0.2 & -5 & 3 & 0.4 & 0\\end{bmatrix} $\n",
    "\n",
    "#### Infinity Norm\n",
    "This is equal to the largest magnitude element of $[v]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}v\\end{Vmatrix}_{\\infty} = max( |v_i| )~~i=0,1,2,...,n-1$ \n",
    "\n",
    "For the example vector, $[v]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}v\\end{Vmatrix}_{\\infty} = 5$ \n",
    "\n",
    "#### 1-norm\n",
    "This is the sum of the magnitudes of all vector elements:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}v\\end{Vmatrix}_{1} = \\displaystyle\\sum_{i=0}^{i=n-1} |v_i|$ \n",
    "\n",
    "For the example vector, $[v]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}v\\end{Vmatrix}_{1} = \\displaystyle\\sum_{i=0}^{i=n-1} |v_i| = 0.2+5+3+0.4+0 = 8.6$ \n",
    "\n",
    "#### Euclidean norm\n",
    "This is the square root of the sum of the squares of all vector elements:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}v\\end{Vmatrix}_{2} = \\sqrt{ \\displaystyle\\sum_{i=0}^{i=n-1} v_i^2 }$ \n",
    "\n",
    "For the example vector, $[v]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}v\\end{Vmatrix}_{2} = \\sqrt{ \\displaystyle\\sum_{i=0}^{i=n-1} v_i^2 } = \\sqrt{0.2^2+(-5)^2+3^2+0.4^2+0^2} = \\sqrt{34.2} = 5.8 $ \n",
    "\n",
    "### Calculating Matrix Norms\n",
    "\n",
    "Assume we have a matrix $[a]$. Below we go through some norms and show an example calculation of each. For these examples we will use\n",
    "\n",
    "$ \\large [a] = \\begin{bmatrix} 0.2 & -5 & 3 & 0.4 & 0 \\\\-0.5 & 1 & 7 & -2 & 0.3 \\\\ 0.6 & 2 & -4 & 3 & 0.1 \\\\ 3 & 0.8 & 2 & -0.4 & 3 \\\\ 0.5 & 3 & 2 & 0.4 & 1   \\end{bmatrix} $\n",
    "\n",
    "\n",
    "#### Infinity Norm\n",
    "This is equal to the largest value when the magnitude of each element in a row is summed up of $[a]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}a\\end{Vmatrix}_{\\infty} = max( \\displaystyle\\sum_{j=0}^{j=n-1} |a_{ij}| ) $\n",
    "\n",
    "For the example vector, $[a]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}a\\end{Vmatrix}_{\\infty} = 0.5+1+7+2+0.3 = 10.8 $ \n",
    "\n",
    "#### 1-Norm\n",
    "This is equal to the largest value when the magnitude of each element in a column is summed up of $[a]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}a\\end{Vmatrix}_{1} = max( \\displaystyle\\sum_{i=0}^{i=n-1} |a_{ij}| ) $\n",
    "\n",
    "For the example vector, $[a]$:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}a\\end{Vmatrix}_{1} = 3+7+4+2+2 = 18 $ \n",
    "\n",
    "#### Euclidean Norm\n",
    "This is equal to the square root of the sum of the squares of all matrix elements. Note this does not have to be a square matrix. In general this is for a $mxn$ matrix:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}a\\end{Vmatrix}_{Euclidean} = \\sqrt{ \\displaystyle\\sum_{i=0}^{i=m-1} \\displaystyle\\sum_{j=0}^{j=n-1} a_{ij}^2 } $\n",
    "\n",
    "For the example vector, $[a]$:\n",
    "\n",
    "$ \\begin{Vmatrix}a\\end{Vmatrix}_{Euclidean} = \\sqrt{0.2^2+(-5)^2+3^2+0.4^2+0^2+(-0.5)^2+1^2+7^2+(-2)^2+0.3^2+0.6^2+2^2+(-4)^2+3^2+0.1^2+3^2+0.8^2+2^2+(-0.4^2)+3^2+0.5^2+3^2+2^2+0.4^2+1^2} = \\sqrt{154.8} = 12.4 $\n",
    "\n",
    "### Condition Numbers\n",
    "\n",
    "Condition numbers are used to assess to what precision can we expect to be able to solve a matrix - in quantitative terms what will the possible range of the relative error be in our solution. But before we define *condition number* we need just a little context.\n",
    "\n",
    "Through a whole lot of matrix algebra and the rules regarding *norms* it can be shown that:\n",
    "\n",
    "$\\large \\frac{1}{\\begin{Vmatrix}[a]\\end{Vmatrix}\\begin{Vmatrix}[a]^{-1}\\end{Vmatrix} } \\frac{\\begin{Vmatrix}[r]\\end{Vmatrix}}{\\begin{Vmatrix}[b]\\end{Vmatrix}} \\le \\frac{\\begin{Vmatrix}[e]\\end{Vmatrix}}{\\begin{Vmatrix}[x_{TS}]\\end{Vmatrix}} \\le \\begin{Vmatrix}[a]\\end{Vmatrix}\\begin{Vmatrix}[a]^{-1}\\end{Vmatrix} \\frac{\\begin{Vmatrix}[r]\\end{Vmatrix}}{\\begin{Vmatrix}[b]\\end{Vmatrix}} $\n",
    "\n",
    "It's kind of a lot to take in, but... let's simplfy it. First, let's define the *condition number*:\n",
    "\n",
    "$ \\large Cond([a]) = \\begin{Vmatrix}[a]\\end{Vmatrix}\\begin{Vmatrix}[a]^{-1}\\end{Vmatrix} $\n",
    "\n",
    "Next, the *relative error* for the solution, $[x]$:\n",
    "\n",
    "$ \\large \\epsilon_{rel,x} = \\frac{\\begin{Vmatrix}[e]\\end{Vmatrix}}{\\begin{Vmatrix}[x_{TS}]\\end{Vmatrix}} $\n",
    "\n",
    "And the relative error in the residual, $[r]$:\n",
    "\n",
    "$ \\large \\epsilon_{rel,r} =\\frac{\\begin{Vmatrix}[r]\\end{Vmatrix}}{\\begin{Vmatrix}[b]\\end{Vmatrix}}$\n",
    "\n",
    "So the inequality above can be rewritten as:\n",
    "\n",
    "$ \\large \\frac{\\epsilon_{rel,r}}{Cond([a])} \\le \\epsilon_{rel,x} \\le Cond([a]) \\epsilon_{rel,r} $\n",
    "\n",
    "So the way to think about this inequality is that we can actually calculate the bounds of the relative error for the solution, $\\epsilon_{rel,x}$. The leftmost expression is the lower bound on $\\epsilon_{rel,x}$ and the righmost expression is the upper bound on $\\epsilon_{rel,x}$. A concrete example is given below. But first let's say a little more about $Cond([a])$.\n",
    "\n",
    "Probably the biggest thing is that if $Cond([a])$ is near $1$, then the error bounds will be very small... according to our inequality (the smallest that $Cond([a])$ can be is 1, by the way). If $Cond([a]) >> 1$ then the error bounds will be large.\n",
    "\n",
    "- $Cond([I]) = 1$ and $Cond([a]) > 1$ for all other matrices.\n",
    "- When $Cond([a]) \\approx 1$ then $ \\large \\epsilon_{rel,x} \\approx \\epsilon_{rel,r}$\n",
    "\n",
    "    - which is very helpful, since we can't really calculate $ \\large \\epsilon_{rel,x}$, but we can calculate $ \\large \\epsilon_{rel,r}$.\n",
    "    \n",
    "- If the $Cond([I]) >> 1$, let's say $Cond([I])=10$, and as an example, $\\epsilon_{rel,r} = 0.1$, then\n",
    "    - $ \\frac{1}{100} \\le \\epsilon_{rel,x} \\le 1$, which is a large uncertainty in the relative error.\n",
    "    - Restating this... this is *big uncertainy*! Let's express this in percentages:\n",
    "    - $ 1\\% \\le \\epsilon_{rel,x} \\le 100\\%$!!\n",
    "    - Those are very large error bounds... literally the relative error in your solution could be any where from 1% to 100% !\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Gauss Siedel Condtion Number Example\n",
    "\n",
    "$ \\large [a] = \\begin{bmatrix} 0.2 & -5 & 3 & 0.4 & 0 \\\\-0.5 & 1 & 7 & -2 & 0.3 \\\\ 0.6 & 2 & -4 & 3 & 0.1 \\\\ 3 & 0.8 & 2 & -0.4 & 3 \\\\ 0.5 & 3 & 2 & 0.4 & 1   \\end{bmatrix} $\n",
    "\n",
    "$ \\large [a]^{-1} = \\begin{bmatrix} −0.7079 & 2.5314 &2.4312& 0.9666 &−3.9023 \\\\ −0.1934 & 0.3101& 0.2795& 0.0577& −0.2941 \\\\ 0.0217& 0.3655& 0.2861& 0.0506& −0.2899 \\\\ 0.2734& −0.1299& 0.1316& −0.1410& 0.4489 \\\\ 0.7815& −2.8751& −2.6789& −0.7011& 4.2338 \\end{bmatrix} \\leftarrow $ Just assume you have this from somewhere...\n",
    "\n",
    "Using the *infinity norm*:\n",
    "\n",
    "$ \\large \\begin{Vmatrix}a\\end{Vmatrix}_{\\infty} = 0.5+1+7+2+0.3 = 10.8$ \n",
    "\n",
    "$ \\large \\begin{Vmatrix}a\\end{Vmatrix}_{\\infty}^{-1} = 0.7815 + 2.8751 + 2.6789 + 0.7011 + 4.2338 = 11.3$ \n",
    "\n",
    "$ \\large Cond([a]) = \\begin{Vmatrix}[a]\\end{Vmatrix}\\begin{Vmatrix}[a]^{-1}\\end{Vmatrix} = 10.8 x 11.3 = 122$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
