{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Solution of Systems of Linear Equations\n",
    "\n",
    "## Background\n",
    "\n",
    "### Engineering Problems\n",
    "\n",
    "We can write many engineering problems as systems of linear equations, which happen to have many ways of being solved numerically. Here are some general engineering problems that can be written as linear systems:\n",
    "\n",
    "1. Truss Problems (Newton's Laws)\n",
    "2. Circuit Problems (Kirchoff's Laws)\n",
    "3. Motion of Objects with Internal Connections (Newton's laws - written as ODEs)\n",
    "4. Stress/Strain/Deformation (PDEs)\n",
    "5. Fluid Motion (PDEs)\n",
    "\n",
    "### Cramer's Rule\n",
    "\n",
    "When you have a smaller problem, you can usually use Cramer's Rule to solve it. The problem we want to solve is:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\n",
    "\\label{eq1}\\tag{1} $\n",
    "\n",
    "Cramer's Rule States:\n",
    "\n",
    "$ \\large x_0 =  \\frac{\\begin{vmatrix} b_0 & a_{01} & a_{02} \\\\ b_1 & a_{11} & a_{12} \\\\ b_2 & a_{21} & a_{22} \\end{vmatrix}}{\\begin{vmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{vmatrix} }~~~  x_1 =  \\frac{\\begin{vmatrix} a_{00} & b_0 & a_{02} \\\\ a_{10} & b_1 & a_{12} \\\\ a_{20} & b_2 & a_{22} \\end{vmatrix}}{\\begin{vmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{vmatrix} } ~~~  x_2 =  \\frac{\\begin{vmatrix} a_{00} & a_{01} & b_0 \\\\ a_{10} & a_{11} & b_1 \\\\ a_{20} & a_{21} & b_2 \\end{vmatrix}}{\\begin{vmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{vmatrix} }  \\label{eq2}\\tag{2} $\n",
    "\n",
    "I will try to stick with $0$ as the starting subscript. Since we are coding in python that will work much better for the arrays and loop bounds.\n",
    "\n",
    "### Triangular Forms\n",
    "\n",
    "#### Upper Triangulars\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ 0 & a_{11} & a_{12} \\\\ 0 & 0 & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq3}\\tag{3}$\n",
    "\n",
    "An upper triangular problem like this can be solved using *back substitution*.\n",
    "\n",
    "Although not as common another upper triangular can be written as:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & 0 \\\\ a_{20} & 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq4}\\tag{4}$\n",
    "\n",
    "#### Lower Triangulars\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & 0 & 0 \\\\ a_{10} & a_{11} & 0 \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq5}\\tag{5}$\n",
    "\n",
    "And yet again there is an alternative (somewhat uncommon) way of writing another lower triangular:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 0 & 0 & a_{02} \\\\ 0 & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq6}\\tag{6}$\n",
    "\n",
    "#### Diagonal Matrix\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & 0 & 0 \\\\ 0 & a_{11} & 0 \\\\ 0 & 0 & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}\\label{eq7}\\tag{7}$\n",
    "\n",
    "A diagonal matrix is easy to solve because $ \\large x_i = \\frac{b_i}{a_{ii}}$.\n",
    "\n",
    "A very special diagonal matrix is the identity matrix:\n",
    "\n",
    "$ \\large I = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "## Back Substitution - Algorithm\n",
    "\n",
    "The technique known as *back substitution* can be used to find $[x]$ in the case of Eq.($\\ref{eq3}$) - which we will now demonstrate. For Eq.($\\ref{eq3}$) from the last row of the matrix we can write:\n",
    "\n",
    "$\\large x_2 = \\frac{b_2}{a_{22}} \\label{eq8}\\tag{8}$. Since $x_2$ is now known, we can find $x_1$ as\n",
    "\n",
    "$\\large x_1 = \\frac{b_1 - a_{12} x_2}{a_{11}} \\label{eq9}\\tag{9} $ and now we can find $x_0$ as\n",
    "\n",
    "$\\large x_0 = \\frac{b_0 - a_{01} x_1 - a_{02} x_2}{a_{00}} = \\frac{b_0 - (a_{01} x_1 + a_{02} x_2)}{a_{00}} = \\frac{b_0 - \\sum\\limits_{j=1}^{2} a_{0j} x_j}{a_{00}} \\label{eq10}\\tag{10} $\n",
    "\n",
    "For that matter, $x_1$ could have been written as \n",
    "\n",
    "$\\large x_1 = \\frac{b_1 - \\sum\\limits_{j=2}^{2} a_{1j} x_j}{a_{11}} \\label{eq11}\\tag{11} $\n",
    "\n",
    "Of course there will be only one term in the summation here. So we now almost have an algorithm... we just need to generalize for $x_i$ and determine the generalized summation limits. We will just use the simple calculation of $x_2$ to get the process started and that the size of the problem/martrix is $nxn$.\n",
    "\n",
    "In looking at Eq.($\\ref{eq10}$) where $i=0$ it seems that *j* ranges from $i+1 = 1$ to $n-1 = 2$ So the general form of the expression for finding $x_i$ would be\n",
    "\n",
    "$\\large x_i =  \\frac{b_i - \\sum\\limits_{j=i+1}^{n-1} a_{ij} x_j}{a_{ii}}~~~ i=n-2, n-3, ...0 \\label{eq12}\\tag{12} $\n",
    "\n",
    "Again, this assumes that we have already calculated:\n",
    "\n",
    "$\\large x_{n-1} = \\frac{b_{n-1}}{a_{n-1,n-1}} \\label{eq13}\\tag{13}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Elimination (including Forward Elimination, Normalization and Back Substitution)\n",
    "\n",
    "Returning to the original problem:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix} $\n",
    "\n",
    "Now we will demonstrate the Gauss elimination technique with an example. The *pivots* are the diagonal elements and a *pivot* row is a row containing a pivot. The process starts at the top row, eliminates all elements below the current pivot, then moves to the next row.\n",
    "\n",
    "$ \\large x_1 - 3 x_2 + x_3 = 4 $\n",
    "\n",
    "$ \\large 2 x_1 - 8 x_2 + 8 x_3 = -2 $\n",
    "\n",
    "$ \\large -6 x_1 + 3 x_2 - 15  x_3 = 9 $\n",
    "\n",
    "Or written in matrix form.\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 \\\\ 2 & -8 & 8 \\\\ -6 & 3 & -15 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -2 \\\\ 9 \\end{bmatrix} $\n",
    "We usually write this in an abbreviated form that is convenient for Gauss Elimination: \n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 2 & -8 & 8 & -2 \\\\ -6 & 3 & -15 & 9 \\end{bmatrix}$\n",
    "\n",
    "This is the *augmented matrix* form. It's convenient because we are going to do operations on each row (including the RHS vector).\n",
    "\n",
    "### Forward Elimination\n",
    "\n",
    "The process is basically this: multiply the pivot row by negative of the element below the pivot you want to eliminate... then add this multiplied pivot row to the row in which you are eliminating an element... the result of this addition is the new row. For example if the pivot is the *1* in position $a_{00}$, then to get rid of the *2* in position $a_{10}$, we would multiply row *0* by *-2* then add that to row *1*. Below I will use an $R_i$ to represent the ith row.\n",
    "\n",
    "$ \\large -2 R_0 + R_1 = R_{1}^{'} $\n",
    "\n",
    "$ \\large -2 \\begin{bmatrix} 1 & -3 & 1 & 4 \\end{bmatrix} = \\begin{bmatrix} -2 & 6 & -2 & -8 \\end{bmatrix} \\rightarrow \\begin{bmatrix} -2 & 6 & -2 & -8 \\end{bmatrix} + \\begin{bmatrix} 2 & -8 & 8 & -2 \\end{bmatrix} = \\begin{bmatrix} 0 & -2 & 6 & -10 \\end{bmatrix} = R_{1}^{'}$\n",
    "\n",
    "Now the augmented matrix looks as follows:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & -2 & 6 & -10 \\\\ -6 & 3 & -15 & 9 \\end{bmatrix}$\n",
    "\n",
    "Now we can eliminate the *-6* in $R_2$ by the following operation: $ 6 R_0 + R_2 = R_{2}^{'} $, which gives:\n",
    "\n",
    "$ \\large 6 \\begin{bmatrix} 1 & -3 & 1 & 4 \\end{bmatrix} = \\begin{bmatrix} 6 & -18 & 6 & 24 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 6 & -18 & 6 & 24 \\end{bmatrix} + \\begin{bmatrix} -6 & 3 & 15 & 9 \\end{bmatrix} = \\begin{bmatrix} 0 & -15 & -9 & 33 \\end{bmatrix} = R_{2}^{'}$\n",
    "\n",
    "So the new version of the augmented matrix looks like:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & -2 & 6 & -10 \\\\ 0 & -15 & -9 & 33 \\end{bmatrix}$\n",
    "\n",
    "The next step is use $R_{1}^{'}$ as the pivot row, but before doing that we *normalize* $R_{1}^{'}$... which means to make the pivot element - the *-2* - a *1* ... which we can do by taking $\\frac{R_{1}^{'}}{-2} = \\begin{bmatrix} 0 & 1 & -3 & 5 \\end{bmatrix} = R_{1}^{''}$ which leaves our matrix as\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & 1 & -3 & 5 \\\\ 0 & -15 & -9 & 33 \\end{bmatrix}$\n",
    "\n",
    "Now eliminate the *-15* below the current pivot by $ 15 R_{1}^{''} + R_{2}^{'} $, which gives:\n",
    "\n",
    "$ \\large 15 \\begin{bmatrix} 0 & 1 & -3 & 5 \\end{bmatrix} = \\begin{bmatrix} 0 & 15 & -45 & 75 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 0 & 15 & -45 & 75 \\end{bmatrix} + \\begin{bmatrix} 0 & -15 & -9 & 33 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & -54 & 108 \\end{bmatrix} = R_{2}^{''}$\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & 1 & -3 & 5 \\\\ 0 & 0 & -54 & 108 \\end{bmatrix}$\n",
    "\n",
    "Let's go ahead and normalize the last row - $\\frac{R_{2}^{''}}{-54} = \\begin{bmatrix} 0 & 0 & 1 & -2 \\end{bmatrix} = R_{2}^{'''}$.\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & -3 & 1 & 4 \\\\ 0 & 1 & -3 & 5 \\\\ 0 & 0 & 1 & -2 \\end{bmatrix}$\n",
    "\n",
    "At this point *forward elimination* is complete!\n",
    "\n",
    "### Back Substitution\n",
    "\n",
    "The equation form of the last row $R_{2}^{'''}$ is:\n",
    "\n",
    "$ \\large (1)x_2 = -2 \\rightarrow x_2 = -2 $\n",
    "\n",
    "Allowing us to solve the equation form of $R_{1}^{''}$:\n",
    "\n",
    "$ \\large (1) x_1 + (-3)(-2) = 5 \\rightarrow x_1 = -1$\n",
    "\n",
    "And finally we can solve the equation from $R_{0}^{'}$ as\n",
    "\n",
    "$ \\large (1) x_0 + (-3)(-1) + (1)(-2) = 4  \\rightarrow x_0 = 3$\n",
    "\n",
    "Giving the final solution as:\n",
    "\n",
    "$ \\large x = \\begin{bmatrix} 3 \\\\ -1 \\\\ -2 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-integer example\n",
    "\n",
    "We will now do an example that cannot be done only with integers... in fact we will limit the number of decimal places so we can see the effects.\n",
    "\n",
    "$ \\large \\begin{bmatrix} 0.143 & 0.357 & 2.01 \\\\ -1.31 & 0.911 & 1.99 \\\\ 11.2 & -4.30 & -0.605 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} -5.173 \\\\ -5.458 \\\\ 4.415 \\end{bmatrix} $ In augmented form: $ \\large \\begin{bmatrix} 0.143 & 0.357 & 2.01 & -5.173\\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} $\n",
    "\n",
    "\n",
    "Now the steps of Gauss Elimination would go as follows:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 4.19 & 20.5 & -52.9 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 4.19 & 20.5 & -52.9 \\\\ 0.00 & -32.3 & 159. & 409 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 1.00 & 4.89 & -12.6 \\\\ 0.00 & -32.3 & 159. & 409 \\end{bmatrix} $\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 1.00 & 4.89 & -12.6 \\\\ 0.00 & 0.00 & -1.00 & 2.00 \\end{bmatrix} $\n",
    "\n",
    "And finally:\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1.00 & 2.50 & 14.1 & -36.2 \\\\ 0.00 & 1.00 & 4.89 & -12.6 \\\\ 0.00 & 0.00 & 1.00 & -2.00 \\end{bmatrix} $\n",
    "\n",
    "Now backsubstition:\n",
    "\n",
    "$ \\large  x_2 = -2.00~~~ x_1 + 4.89(-2.0) = -12.6 \\rightarrow x_1 = -2.82~~~ x_0 + 2.50(-2.82) + 14.1(-2.00) = -36.2 \\rightarrow x_0 = -0.950 $\n",
    "\n",
    "Like a good engineer - we should check our answers, but we should use our original equations:\n",
    "\n",
    "\n",
    "$ \\large 0.143(-0.950) + 0.357(-2.82) + 14.1(-2.00) = -29.3 \\ne -5.173$ Oops... this is not a good sign.\n",
    "\n",
    "$ \\large -1.31(-0.950) + 0.911(-2.82) + 1.99(-2.00) = -5.30 \\ne -5.458$\n",
    "\n",
    "$ \\large 11.2(-0.950) + (-4.30)(-2.82) + (-0.605)(-2.00) =  2.70 \\ne 4.415$\n",
    "\n",
    "So - what happened? The limited precision of the numbers throughout the process caused significant rounding error. Although we kept the rounding to three digits - a pretty significant loss of precision - we were only working a 3x3 problem. Imagine the effects one could encounter in a much larger system of equations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Elimination Algorithm\n",
    "\n",
    "As understandable as the forward elimination algorithm might seem to be in doing a problem by hand... to automate the process (and then finally write code to implement it) is not trivial. To help understand how you can take a complicated process and automate it, it's often easier to write out an example problem - but to start to understand what parts need to be repeated and since there will be indicies for the array items we need to understand the patterns that appear in the algorithm with indicies. To do this here is a written document that shows how to think through the process for *forward elimination.*\n",
    "\n",
    "[Forward Elimination Algorithm Development](forward_elim_alg_dev.pdf).\n",
    "\n",
    "Note that the algorithm development document does not address how to move from one row to the next as the pivot row changes... it also does not address normalizing these additional pivot rows as we move down through the matrix.\n",
    "\n",
    "This assumes we are starting our indices at 0 and the original problem to be solved is $n~x~n$. Also this assumes we have augmented the nxn matrix with the RHS vector, so the matrix we are working on is actually $n~x~(n+1)$.\n",
    "\n",
    "- *i* index for pivot rows... $ i=0,1,...,n-2$ (Note the row index $n-2$ is the next to last row... and that is where we stop the elimination step).\n",
    "     - *j* index is for rows below *i* in which we are eliminating. $ j=i+1, i+2, ..., n-1 $\n",
    "         - *k* indicates the column in which we are *modifying* elements within row *j*. $ k = i, i+1, i+2, ... , n$\n",
    "             - The replacement/modified element in row *j* for each *k* is calculated as\n",
    "                 $\\large a_{jk} = a_{jk} - a_{ji} a_{ik} $\n",
    "                 - Note that here that the $a_{jk}$ that appears on the RHS of this expression is the *old* value of $a_{jk}$ that we are replacing. In code form this expression will be:\n",
    "                 - ```python\n",
    "                 a[j][k] -= a[j][i]*a[i][k]\n",
    "                 ```\n",
    "             - This process needs to be done all the way to $k=n$ since we are operating on the augmented matrix, which has $n+1$ columns.\n",
    "\n",
    "The above algorithm suggests a series of three nested loops. We do have a missing piece... *normalization* of the pivots. So for the moment let's assume we have written a function to do the normalization of row *i*. The *pseudo-code* below will not actually run in python... it's just a step closed to the actual program.\n",
    "\n",
    "```python\n",
    "#ab = defined as the augmented matrix\n",
    "for i = 0 to n-2\n",
    "    normalize(ab,i)\n",
    "    for j = i+1 to n-1\n",
    "        for k = i to n\n",
    "            #note we may have issues if we start at k = i...\n",
    "            #k=i is the column where the element is being eliminated\n",
    "            #and the following step may not make the new element exactly zero...\n",
    "            ab[j][k] -= ab[j][i]*ab[i][k]\n",
    "```\n",
    "An alternative to get around the issue of the element to be eliminated not being exactly zero is:\n",
    "\n",
    "```python\n",
    "#ab = defined as the augmented matrix\n",
    "for i = 0 to n-2\n",
    "    normalize(ab,i)\n",
    "    for j = i+1 to n-1\n",
    "        ab[j][i] = 0.0 #force the element you are trying to eliminate to be exactly 0.0\n",
    "        for k = i+1 to n\n",
    "            ab[j][k] -= ab[j][i]*ab[i][k]\n",
    "```\n",
    "Either technique can be used and it will make only small differences (keep in mind those small differences might add up to bigger differences if repeated enough times...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Elimination with Partial Pivoting\n",
    "\n",
    "When we worked the following example, it was discussed that we ideally want pivot elements that have the largest magnitude as possible... this because in advance we know we will be dividing the entire pivot row by this value to normalize... *and as we have discussed many times... we never knowingly divide by a small number!!* So what we really should do is as we move to each new pivot row, to find the best possible row (one with the largest magnitude in the pivot position) and swap rows -- this is called **partial pivoting.**\n",
    "\n",
    "$ \\large \\begin{bmatrix} 0.143 & 0.357 & 2.01 & -5.173\\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 11.2 & -4.30 & -0.605 & 4.415 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 11.2 & -4.30 & -0.605 & 4.415\\\\ -1.31 & 0.911 & 1.99 & -5.458 \\\\ 0.143 & 0.357 & 2.01 & -5.173 \\end{bmatrix}  $\n",
    "\n",
    "After the swap shown above, we would normalize row $0$ then eliminate the elements below our pivot. Note once we move to the next pivot row, * we would repeat the process of searching for the best possible pivot at that time. So the partial pivoting process is not done once at the first, but needs to be repeated to find the largest magnitude pivot values each time we shift to a new pivot row.\n",
    "\n",
    "The question is how would we automate this process? There are two parts to this process...\n",
    "\n",
    "1. Identifying the best pivot - searching the pivot column to identify the largest magnitude value,\n",
    "2. \"Swapping\" rows - but should we actually move elements around in the array? The answer is no that would take too much time, in general... instead we will use a vector to keep track of which row we want to \"pretend\" is in each location. We will maintain this pretense with a vector.\n",
    "\n",
    "Here we return to a general form of a $3x3$ problem with the augmented matrix:\n",
    "\n",
    "$ \\large \\begin{bmatrix} ab_{00} & ab_{01} & ab_{02} & ab_{03} \\\\ ab_{10} & ab_{11} & ab_{12} & ab_{13} \\\\ ab_{20} & a_{21} & ab_{22} & ab_{23}\\end{bmatrix} $\n",
    "\n",
    "### Searching for the *Best Value*\n",
    "\n",
    "Let's look specifically at the problem above before any row swapping has occurred. How do you determine which row to switch with? You looked at all elements in the $0$th column and noticed which had the largest magnitude. In pseudocode form:\n",
    "\n",
    "```python\n",
    "max = ab[0][0]   #start by assuming the current pivot is the largest\n",
    "for l = 1 to 2   #look in all rows below the current row\n",
    "    if abs(ab[l][0]) > max\n",
    "        max = abs(ab[l][0])\n",
    "```\n",
    "At the end of this process you would have the correct value in *max,* but you would not have retained which row it came from... so now we have to address keeping track of which row is which.\n",
    "\n",
    "### The *order* Vector\n",
    "\n",
    "We now will make a vector that kept track of the rows. The way it works is that the vector starts as just:\n",
    "\n",
    "$ \\large order=[0, 1, 2] $\n",
    "\n",
    "If we need to swap rows, we will change the $order$ vector to reflect this. For example, for the swap in this example: placing row $0$ in row $2$'s position, the order vector should become:\n",
    "\n",
    "$ \\large order=[2, 1, 0] $\n",
    "\n",
    "So now the index $2$ is in the $0$th position and the index $0$ is in the $2$-row position. How do we accomplish this via pseudocode?\n",
    "\n",
    "```python\n",
    "max = ab[0][0]                    #start by assuming the current pivot is the largest\n",
    "order = [0,1,2]                   #define the order vector\n",
    "index = order[0]                  #this is keeping track of which row is best at the moment\n",
    "for l = 1 to 2                    #look in all rows below the current row\n",
    "    if abs(ab[l][0]) > max\n",
    "        max = abs(ab[l][0])\n",
    "        index = l\n",
    "tmp = order[0]                    #temporarily store order[0] here\n",
    "order[0] = order[index]           #replace order[0] with the best pivot row\n",
    "order[index] =  tmp               #place the original pivot row in the location where the best pivot row had been\n",
    "```\n",
    "\n",
    "The next step is to generalize this so it will work for any pivot row. We have used the index *i* for the current pivot row, so we will continue with this notation now. Note, here we assume the *order* vector is in place (but note, it could have been previously modified).\n",
    "\n",
    "```python\n",
    "max = ab[i][i]                    #start by assuming the current pivot is the largest\n",
    "index = order[i]                  #this is keeping track of which row is best at the moment\n",
    "for p = i+1 to n-1                    #look in all rows below the current row\n",
    "    if abs(ab[p][i]) > max\n",
    "        max = abs(ab[p][i])\n",
    "        index = p\n",
    "tmp = order[i]                    #temporarily store order[0] here\n",
    "order[i] = order[index]           #replace order[0] with the best pivot row\n",
    "order[index] =  tmp               #place the original pivot row in the location where the best pivot row had been\n",
    "```\n",
    "\n",
    "### How Partial Pivoting Works\n",
    "\n",
    "So we now have a pathway to make sure we know which is the best pivot row, but how do we actually use this with *forward elimination*, *normalization*, and *back substitution*? The answer is we keep up the pretense that we have actually swapped rows by dealing with the following matrix (instead of the original we were given):\n",
    "\n",
    "$ \\large \\begin{bmatrix} ab_{order[0],0} & ab_{order[0],1} & ab_{order[0],2} & ab_{order[0],3} \\\\ ab_{order[1],0} & ab_{order[1],1} & ab_{order[1],2} & ab_{order[1],3} \\\\ ab_{order[2],0} & a_{order[2],1} & ab_{order[2],2} & ab_{order[2],3}\\end{bmatrix} $\n",
    "\n",
    "In short, instead of using the actual row index, we use the what is in the *order* vector in the location for the actual row index. So any operation on the augmented matrix, is done with the above version of the matrix. Note the changes to any code for *forward elimination*, *normalization*, and *back substitution* are minimal and amount to replacing row indicies with the *order vector* referenced to the actual row index.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Jordan Elimination\n",
    "\n",
    "If we use forward and *back* elimination combined with normalization, it is possible to transform the original coefficient matrix, $a$ into the identity matrix. Then the modified RHS vector, $b$ will actually be the solution.\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} \\\\ a_{10} & a_{11} & a_{12} \\\\ a_{20} & a_{21} & a_{22} \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} b_{0}^{'} \\\\ b_{1}^{'} \\\\ b_{2}^{'} \\end{bmatrix} $\n",
    "\n",
    "So now the $b^{'}$ vector contains the solution.\n",
    "\n",
    "The algorithm to do Gauss-Jordan is very similar to forward elimination - one just needs to do elimination in rows that are *above and below* the pivot row. Note one important attribute of this technique is that *back substitution is not required!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU Decomposition\n",
    "\n",
    "*Lower - Upper* or LU decomposition is useful for some problems, especially ones in which the same coefficient matrix is used over and over with a different RHS vector. Although there are some different ways of doing LU decomposition we are focused on Crout's method which decomposes the coefficient matrix into a lower triangular and an upper triangular as:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} & a_{03} \\\\ a_{10} & a_{11} & a_{12} & a_{13}\\\\ a_{20} & a_{21} & a_{22} & a_{23}\\\\  a_{30} & a_{31} & a_{32} & a_{33} \\end{bmatrix} = \\begin{bmatrix} L_{00} & 0 & 0 & 0\\\\ L_{10} & L_{11} & 0 & 0 \\\\ L_{20} & L_{21} & L_{22} & 0 \\\\L_{30} & L_{31} & L_{32} & L_{33} \\end{bmatrix}  \\begin{bmatrix} 1 & U_{01} & U_{02} & U_{03} \\\\ 0 & 1 & U_{12} & U_{13} \\\\ 0 & 0 & 1 &U_{23} \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $\n",
    "\n",
    "By carrying out the matrix multiplication on the RHS of the above equation ine can find how the elements of $a$ are related to $L$ and $U$. The details of this are here:\n",
    "\n",
    "[Crout Method Algorithm Development](crout_decomp_alg_dev.pdf).\n",
    "\n",
    "One the attributes of this algorithm is that in must proceed in a certain order to ensure that the correct $L$ and $U$ elements are known in time for calculation of other $L$ and $U$ elements. The last page of the algorithm development document shows this graphically, but putting this into pseudocode:\n",
    "\n",
    "```python\n",
    "for j = 0 to n-1\n",
    "    L[j][0] = a[j][0]\n",
    "for i = 0 to n-1\n",
    "    U[i][i] = 1.0\n",
    "for i = 0 to n-1\n",
    "    for j = 0 to n-1\n",
    "        #if left of the diagonal or on the diagonal\n",
    "            L[i][j] = a[i][j] - get_sum(L,U,i,j)\n",
    "        #if right of the diagonal\n",
    "            U[i][j] = (a[i][j] - get_sum(L,U,i,j))/L[i][i]\n",
    "```\n",
    "In the above pseudocode it is assumed you have a function called *get_sum* that calculates: $ \\large \\displaystyle\\sum_{k-0}^{j-1} L_{ik} U_{kj} $\n",
    "\n",
    "### Using LU Decomposition to solve $[a][x]=[b]$\n",
    "\n",
    "You may have noticed that LU decomposition does not account for finding $[x]$ and that no $[b]$ has been specified. As mentioned LU decomposition is good when the same coefficient matrix needs to be used for a number of different $[b]$ RHS vectors. So here is the way one can find $[x]$ once you have $[L]$ and $[U]$.\n",
    "\n",
    "$ \\large [a][x] = [L][U][x] = [b] \\rightarrow [L][y] = [b]$ where $[U][x] = y$\n",
    "\n",
    "$ \\large \\begin{bmatrix} L_{00} & 0 & 0 & 0\\\\ L_{10} & L_{11} & 0 & 0 \\\\ L_{20} & L_{21} & L_{22} & 0 \\\\L_{30} & L_{31} & L_{32} & L_{33} \\end{bmatrix} \\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} = \\begin{bmatrix} b_0 \\\\ b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} $\n",
    "\n",
    "\n",
    "Note $[y]$ can be found by forward elimination -- or better yet a form of *forward substitution* can be used. This is completely analogous to using back substitution, but done from the top to the bottom of the matrix.\n",
    "\n",
    "\n",
    "Now that we have $[y]$:\n",
    "\n",
    "\n",
    "$ \\large \\begin{bmatrix} 1 & U_{01} & U_{02} & U_{03} \\\\ 0 & 1 & U_{12} & U_{13} \\\\ 0 & 0 & 1  & U_{23} \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\end{bmatrix} $\n",
    "\n",
    "The above equation can solved quickly with back substitution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inversion\n",
    "\n",
    "The matrix inverse, $[a]^{-1}$ is defined as:\n",
    "\n",
    "$ \\large [a]^{-1} [a] = [I]$\n",
    "\n",
    "When a matrix inverse is possible to calculate it can be very convenient, but please note: *matrix inversion is fraught with issues related to how much precision can be retained in the calculation when $\\det{a} = 0$ or $\\det{a}$ is relatively small.*\n",
    "\n",
    "There are several techniques used to perform matrix inversion numerically. We will present only one here (without proof). This technique uses Gauss-Jordan Elimination. First $[a]$ is augmented with an indentity matrix of the same size as $[a]$ then Gauss Jordan is used until the original elements of $[a]$ are filled with an identity matrix as shown:\n",
    "\n",
    "$ \\large \\begin{bmatrix} a_{00} & a_{01} & a_{02} & a_{03} & 1 & 0 & 0 & 0 \\\\ a_{10} & a_{11} & a_{12} & a_{13} & 0 & 1 & 0 & 0\\\\ a_{20} & a_{21} & a_{22} & a_{23} & 0 & 0 & 1 & 0 \\\\  a_{30} & a_{31} & a_{32} & a_{33} & 0 & 0 & 0 & 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix}  1 & 0 & 0 & 0 & a_{00}^{'} & a_{01}^{'} & a_{02}^{'} & a_{03}^{'} \\\\  0 & 1 & 0 & 0 & a_{10}^{'} & a_{11}^{'} & a_{12}^{'} & a_{13}^{'}\\\\  0 & 0 & 1 & 0 &  a_{20}^{'} & a_{21}^{'} & a_{22}^{'} & a_{23}^{'} \\\\  0 & 0 & 0 & 1 &  a_{30}^{'} & a_{31}^{'} & a_{32}^{'} & a_{33}^{'} \\end{bmatrix}   $\n",
    "\n",
    "The *primed* elements above are the elements of $[a]^{-1}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
